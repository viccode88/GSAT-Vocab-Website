#!/usr/bin/env python3
"""
ã€å‡ç´šç‰ˆ v3 - Quick Response APIã€‘å­¸æ¸¬è‹±æ–‡æ•¸æ“šé è™•ç†è…³æœ¬

åŠŸèƒ½ï¼š
1. å¾ PDF æå–æ–‡æœ¬ä¸¦å„²å­˜åŸæ–‡ã€‚
2. ä½¿ç”¨ spaCy é€²è¡Œ NLP è™•ç†ï¼Œåˆ†æè©å½¢ (lemma)ã€è©æ€§ (POS) ä¸¦å„²å­˜ä¾‹å¥ã€‚
3. éæ¿¾åœç”¨è©ã€ç„¡æ„ç¾©çš„ç¬¦è™Ÿèˆ‡å¥å­ã€‚
4. (å¯é¸) ä½¿ç”¨ OpenAI Chat Completions APIï¼Œä»¥é«˜æ•ˆç‡çš„ä¸¦ç™¼æ¨¡å¼ç²å–æ‰€æœ‰å–®å­—çš„ä¸­è‹±é‡‹ç¾©èˆ‡ä¾‹å¥ã€‚
5. å°‡æ‰€æœ‰è™•ç†çµæœå½™æ•´æˆä¸€ä»½çµæ§‹åŒ–çš„ JSON æª”æ¡ˆï¼Œä¾›å‰ç«¯ä½¿ç”¨ã€‚
"""
import os
import re
import sys
import json
import asyncio
from pathlib import Path
from collections import defaultdict
from typing import List, Dict, Any, Tuple, Optional

# --- å¤–éƒ¨å‡½å¼åº« ---
import pdfplumber
import spacy
from tqdm.asyncio import tqdm
from dotenv import load_dotenv
from openai import AsyncOpenAI
from openai import RateLimitError, APIError

# --- ç¬¬ 0 æ­¥ï¼šç’°å¢ƒè¨­å®šèˆ‡å¸¸æ•¸ ---

# è·¯å¾‘è¨­å®š
ROOT_DIR = Path(__file__).parent
SRC_DIR = ROOT_DIR / "ceec_english_papers"  # PDF ä¾†æºè³‡æ–™å¤¾
DATA_DIR = ROOT_DIR / "data"
RAW_DIR = DATA_DIR / "raw_txt"
OUTPUT_DIR = DATA_DIR / "output"
DATA_DIR.mkdir(parents=True, exist_ok=True)
RAW_DIR.mkdir(parents=True, exist_ok=True)
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# OpenAI API è¨­å®š
load_dotenv()
OPENAI_ENABLED = bool(os.getenv("OPENAI_API_KEY"))
if OPENAI_ENABLED:
    client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    OPENAI_MODEL = "gpt-4o-mini" # æ¨è–¦ä½¿ç”¨ gpt-4o-miniï¼Œæ€§åƒ¹æ¯”é«˜

# NLP èˆ‡éæ¿¾è¨­å®š
try:
    nlp = spacy.load("en_core_web_sm", disable=["ner"])
except OSError:
    print("Downloading spaCy model en_core_web_smâ€¦")
    os.system(f"{sys.executable} -m spacy download en_core_web_sm")
    nlp = spacy.load("en_core_web_sm", disable=["ner"])

# æ’é™¤çš„è©æ€§
STOP_POS = {"ADP", "AUX", "CONJ", "CCONJ", "DET", "NUM", "PART", "PRON", "SCONJ", "PUNCT", "SPACE", "SYM", "X"}

# è‡ªè¨‚çš„åœç”¨è©åˆ—è¡¨ (å°å¯«)
CUSTOM_STOP_WORDS = {
    'be', 'have', 'do', 'say', 'get', 'make', 'go', 'know', 'take', 'see', 'come', 'think',
    'look', 'want', 'give', 'use', 'find', 'tell', 'ask', 'work', 'seem', 'feel', 'try',
    'leave', 'call', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n',
    'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'to', 'from', 'in', 'on',
    'at', 'for', 'with', 'by', 'as', 'it', 'he', 'she', 'they', 'we'
}

# --- ç¬¬ 1 æ­¥ï¼šå·¥å…·å‡½å¼ ---

def extract_text_from_pdf(pdf_file: Path) -> str:
    """å¾ PDF æª”æ¡ˆä¸­æå–æ‰€æœ‰æ–‡æœ¬ã€‚"""
    try:
        with pdfplumber.open(pdf_file) as pdf:
            return "\n".join(p.extract_text(x_tolerance=2, y_tolerance=2) or "" for p in pdf.pages if p.extract_text())
    except Exception as e:
        print(f"  [Warning] pdfplumber failed on {pdf_file.name}: {e}. Falling back is not implemented in this version.")
        return ""

def is_relevant_sentence(sent_text: str) -> bool:
    """åˆ¤æ–·ä¸€å€‹å¥å­æ˜¯å¦ç‚ºæˆ‘å€‘é—œæ³¨çš„å…§å®¹ã€‚"""
    clean_sent = sent_text.strip().lower()
    if not clean_sent or len(clean_sent.split()) < 5:
        return False
    if re.match(r"^(page \d+|\d+ of \d+|section [ivx]+|questions? \d+-\d+)", clean_sent):
        return False
    if re.match(r"^\([a-d]\)", clean_sent):
        return False
    if sum(1 for char in sent_text if char.isupper()) / len(sent_text.replace(" ", "")) > 0.7 and len(sent_text) > 10:
        return False
    return True

# --- ä¿®æ”¹é–‹å§‹ï¼šä½¿ç”¨æ¨™æº– API é€²è¡Œä¸¦ç™¼è«‹æ±‚ ---

async def fetch_single_definition(
    lemma: str, 
    system_prompt: str, 
    semaphore: asyncio.Semaphore
) -> Tuple[str, Optional[Dict[str, str]]]:
    """
    ç‚ºå–®ä¸€å–®å­—ç²å–é‡‹ç¾©ã€‚ä½¿ç”¨ä¿¡è™Ÿé‡æ§åˆ¶ä¸¦ç™¼æ•¸é‡ã€‚

    è¿”å›: ä¸€å€‹åŒ…å« (lemma, definition_dict) çš„å…ƒçµ„ã€‚å¦‚æœå¤±æ•—å‰‡ definition_dict ç‚º Noneã€‚
    """
    async with semaphore:
        try:
            response = await client.chat.completions.create(
                model=OPENAI_MODEL,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": lemma}
                ],
                response_format={"type": "json_object"},
                temperature=0.2,
                timeout=20.0 # è¨­å®š 20 ç§’è¶…æ™‚
            )
            content = response.choices[0].message.content
            if content:
                definition_data = json.loads(content)
                return lemma, definition_data
            else:
                print(f"\n[Warning] No content returned for '{lemma}'")
                return lemma, None
        except RateLimitError:
            print(f"\n[Warning] Rate limit hit. Retrying for '{lemma}' in 10s...")
            await asyncio.sleep(10)
            # ç°¡å–®é‡è©¦ä¸€æ¬¡
            return await fetch_single_definition(lemma, system_prompt, semaphore)
        except (json.JSONDecodeError, KeyError, IndexError, APIError, Exception) as e:
            print(f"\n[Warning] Could not get or parse definition for '{lemma}': {e}")
            return lemma, None

async def get_definitions_concurrently(lemmas: List[str]) -> Dict[str, Any]:
    """
    ä½¿ç”¨æ¨™æº– Chat Completions APIï¼Œä¸¦ç™¼åœ°ç‚ºæ‰€æœ‰å–®å­—ç”Ÿæˆé‡‹ç¾©ã€‚
    """
    if not OPENAI_ENABLED:
        return {}

    print(f"\n--- Step 2: Fetching definitions for {len(lemmas)} words via concurrent API calls ---")
    
    system_prompt = (
        "You are an expert lexicographer. For the English word provided by the user, "
        "give a concise definition in both Traditional Chinese and English, and one simple example sentence in English. "
        "Respond ONLY with a single, valid JSON object with three keys: 'zh_def' (Traditional Chinese definition), "
        "'en_def' (English definition), and 'example' (English example sentence)."
    )

    # ä½¿ç”¨ asyncio.Semaphore ä¾†é™åˆ¶åŒæ™‚é‹è¡Œçš„å”ç¨‹æ•¸é‡ï¼Œé¿å…è§¸ç™¼é€Ÿç‡é™åˆ¶
    # å¯æ ¹æ“šä½ çš„ OpenAI å¸³æˆ¶é€Ÿç‡é™åˆ¶ï¼ˆTierï¼‰èª¿æ•´ï¼ŒTier 1 å»ºè­° 10-15
    CONCURRENT_REQUESTS = 477
    semaphore = asyncio.Semaphore(CONCURRENT_REQUESTS)

    # å»ºç«‹æ‰€æœ‰è«‹æ±‚ä»»å‹™
    tasks = [fetch_single_definition(lemma, system_prompt, semaphore) for lemma in lemmas]

    all_definitions = {}
    
    # ä½¿ç”¨ tqdm.gather ä¾†é¡¯ç¤ºé€²åº¦æ¢ä¸¦åŸ·è¡Œæ‰€æœ‰ä»»å‹™
    results = await tqdm.gather(*tasks, desc="Fetching definitions")
    
    # è™•ç†çµæœ
    for lemma, definition in results:
        if definition:
            all_definitions[lemma] = definition
            
    return all_definitions

# --- ä¿®æ”¹çµæŸ ---


# --- ç¬¬ 2 æ­¥ï¼šä¸»æµç¨‹ ---
async def main():
    """ä¸»åŸ·è¡Œå‡½å¼"""
    print("ğŸš€ Starting enhanced vocabulary processing...")

    pdf_files = sorted(SRC_DIR.rglob("*.pdf"))
    if not pdf_files:
        sys.exit(f"â›” No PDF files found in '{SRC_DIR}'. Please add exam papers.")

    vocab_data = defaultdict(lambda: {
        "count": 0,
        "pos_dist": defaultdict(int),
        "sentences": set()
    })

    print("\n--- Step 1: Parsing PDFs and analyzing text ---")
    for pdf_file in tqdm(pdf_files, desc="Processing PDFs"):
        raw_text = extract_text_from_pdf(pdf_file)
        (RAW_DIR / f"{pdf_file.stem}.txt").write_text(raw_text, encoding="utf-8")
        
        doc = nlp(raw_text)
        for sent in doc.sents:
            sent_text = sent.text.replace('\n', ' ').strip()
            if not is_relevant_sentence(sent_text):
                continue
            for token in sent:
                lemma = token.lemma_.lower()
                if (token.pos_ not in STOP_POS and
                    token.is_alpha and
                    not token.is_stop and
                    len(lemma) > 1 and
                    lemma not in CUSTOM_STOP_WORDS):
                    vocab_data[lemma]["count"] += 1
                    vocab_data[lemma]["pos_dist"][token.pos_] += 1
                    # åªå„²å­˜å‰ 5 å€‹ä¾‹å¥ï¼Œé¿å…é›†åˆéå¤§æ¶ˆè€—è¨˜æ†¶é«”
                    if len(vocab_data[lemma]["sentences"]) < 5:
                        vocab_data[lemma]["sentences"].add(f"[{pdf_file.stem}] {sent_text}")

    print("\nâœ… Text analysis complete.")
    print(f"ğŸ“Š Found {len(vocab_data)} unique lemmas.")

    sorted_vocab = sorted(vocab_data.items(), key=lambda item: item[1]["count"], reverse=True)
    
    final_data = []
    for lemma, data in sorted_vocab:
        final_data.append({
            "lemma": lemma,
            "count": data["count"],
            "pos_dist": dict(sorted(data["pos_dist"].items(), key=lambda item: item[1], reverse=True)),
            "sentences": list(data["sentences"]),
            "definition": {"zh_def": "", "en_def": "", "example": ""}
        })

    # --- ç¬¬ 3 æ­¥ï¼š(å¯é¸) ä½¿ç”¨ OpenAI API ç²å–é‡‹ç¾© ---
    if OPENAI_ENABLED and final_data:
        words_to_define = [item["lemma"] for item in final_data]
        # ** ä½¿ç”¨æ–°çš„ä¸¦ç™¼å‡½å¼ **
        all_definitions = await get_definitions_concurrently(words_to_define)
        
        if all_definitions:
            print("\n--- Step 3: Merging definitions into final data structure ---")
            # å°‡ç²å–åˆ°çš„é‡‹ç¾©æ›´æ–°å›ä¸»æ•¸æ“šçµæ§‹
            for item in tqdm(final_data, desc="Merging definitions"):
                if item["lemma"] in all_definitions:
                    item["definition"] = all_definitions[item["lemma"]]
            print("\nâœ… Definitions merged successfully.")
    else:
        print("\n--- Step 2: Skipped fetching definitions (OpenAI API key not found or no words to process) ---")

    # --- ç¬¬ 4 æ­¥ï¼šè¼¸å‡ºæœ€çµ‚çš„ JSON æª”æ¡ˆ ---
    output_path = OUTPUT_DIR / "vocab_data.json"
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(final_data, f, indent=2, ensure_ascii=False)

    print(f"\nğŸ‰ All done! Processed data saved to:\n{output_path.resolve()}")


if __name__ == "__main__":
    asyncio.run(main())