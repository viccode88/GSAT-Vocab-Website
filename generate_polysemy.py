#!/usr/bin/env python3
"""
ä¸€å­—å¤šç¾©åˆ†æ
ä½¿ç”¨ OpenAI API åˆ†ææ¯å€‹å–®å­—åœ¨ä¸åŒä¾‹å¥ä¸­çš„èªå¢ƒï¼Œç”Ÿæˆå¤šå€‹æ„æ€
"""
import os
import sys
import json
import time
from pathlib import Path
from typing import Dict, Any, List
from dotenv import load_dotenv
from openai import OpenAI

# --- è·¯å¾‘è¨­å®š ---
ROOT_DIR = Path(__file__).parent
DATA_DIR = ROOT_DIR / "data"
OUTPUT_DIR = DATA_DIR / "output"
BATCH_DIR = DATA_DIR / "batch"
VOCAB_JSON_PATH = OUTPUT_DIR / "vocab_data_filtered_cleaned.json"
BATCH_INPUT_PATH = BATCH_DIR / "polysemy_batch_input.jsonl"
BATCH_OUTPUT_PATH = BATCH_DIR / "polysemy_batch_output.jsonl"
ENRICHED_VOCAB_PATH = OUTPUT_DIR / "vocab_data_filtered_cleaned_enriched.json"

# å»ºç«‹è³‡æ–™å¤¾
BATCH_DIR.mkdir(parents=True, exist_ok=True)

# --- OpenAI API è¨­å®š ---
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    sys.exit("è«‹åœ¨ .env æª”æ¡ˆä¸­è¨­å®šæ‚¨çš„ OPENAI_API_KEYã€‚")

client = OpenAI(api_key=OPENAI_API_KEY)

# --- é…ç½® ---
MODEL = "gpt-4o-mini"
MAX_WORDS_PER_BATCH = 50000  # Batch API é™åˆ¶


def create_batch_input_file(vocab_data: List[Dict[str, Any]]) -> str:
    """
    å‰µå»º Batch API è¼¸å…¥æ–‡ä»¶
    ç‚ºæ¯å€‹å–®è©ç”Ÿæˆä¸€å€‹è«‹æ±‚
    """
    print(f"\nğŸ“ æ­£åœ¨å‰µå»º Batch è¼¸å…¥æ–‡ä»¶...")
    
    system_prompt = """You are an expert lexicographer analyzing English words in context.

Your task: Analyze a word and its example sentences to identify ALL distinct meanings (polysemy).

For each distinct meaning:
1. Identify the part of speech (NOUN, VERB, ADJ, ADV, etc.)
2. Provide a concise Traditional Chinese definition
3. Provide a concise English definition
4. List the indices of example sentences that demonstrate this meaning

Guidelines:
- Consolidate similar meanings into one entry
- Distinguish between different parts of speech
- Distinguish between genuinely different semantic meanings
- Order by frequency (most common first)
- If a word has only one meaning, return one entry

Respond with ONLY a valid JSON object with this structure:
{
  "meanings": [
    {
      "pos": "VERB",
      "zh_def": "ä¸­æ–‡å®šç¾©",
      "en_def": "English definition",
      "example_indices": [0, 2, 5],
      "usage_note": "optional usage note"
    }
  ]
}"""

    batch_requests = []
    request_count = 0
    
    for word_data in vocab_data:
        lemma = word_data["lemma"]
        sentences = word_data.get("sentences", [])
        pos_dist = word_data.get("pos_dist", {})
        
        if not sentences:
            continue
        
        # æ§‹å»ºç”¨æˆ¶æç¤º
        user_prompt = f"""Word: {lemma}

Part of Speech Distribution: {json.dumps(pos_dist)}

Example Sentences:
"""
        for i, sentence in enumerate(sentences):
            # ç§»é™¤æºæ¨™ç±¤ï¼Œåªä¿ç•™å¥å­æ–‡æœ¬
            sentence_text = sentence
            if sentence.startswith('['):
                bracket_end = sentence.find(']')
                if bracket_end != -1:
                    sentence_text = sentence[bracket_end + 1:].strip()
            user_prompt += f"{i}. {sentence_text}\n"
        
        # å‰µå»ºæ‰¹æ¬¡è«‹æ±‚
        batch_request = {
            "custom_id": f"polysemy_{lemma}",
            "method": "POST",
            "url": "/v1/chat/completions",
            "body": {
                "model": MODEL,
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                "response_format": {"type": "json_object"},
                "temperature": 0.3,
                "max_tokens": 2000
            }
        }
        
        batch_requests.append(batch_request)
        request_count += 1
        
        if request_count >= MAX_WORDS_PER_BATCH:
            print(f"âš ï¸  é”åˆ°æ‰¹æ¬¡é™åˆ¶ ({MAX_WORDS_PER_BATCH})ï¼Œå°‡åªè™•ç†å‰ {request_count} å€‹å–®è©")
            break
    
    # å¯«å…¥ JSONL æ–‡ä»¶
    with open(BATCH_INPUT_PATH, 'w', encoding='utf-8') as f:
        for request in batch_requests:
            f.write(json.dumps(request, ensure_ascii=False) + '\n')
    
    print(f"âœ… å·²å‰µå»ºåŒ…å« {len(batch_requests)} å€‹è«‹æ±‚çš„æ‰¹æ¬¡æ–‡ä»¶")
    print(f"   æ–‡ä»¶è·¯å¾‘: {BATCH_INPUT_PATH}")
    
    return str(BATCH_INPUT_PATH)


def upload_batch_file(file_path: str) -> str:
    """ä¸Šå‚³æ‰¹æ¬¡è¼¸å…¥æ–‡ä»¶åˆ° OpenAI"""
    print(f"\nğŸ“¤ æ­£åœ¨ä¸Šå‚³æ‰¹æ¬¡æ–‡ä»¶...")
    
    with open(file_path, 'rb') as f:
        batch_file = client.files.create(
            file=f,
            purpose="batch"
        )
    
    print(f"âœ… æ–‡ä»¶å·²ä¸Šå‚³ï¼ŒFile ID: {batch_file.id}")
    return batch_file.id


def create_batch_job(input_file_id: str) -> str:
    """å‰µå»ºæ‰¹æ¬¡è™•ç†ä»»å‹™"""
    print(f"\nğŸš€ æ­£åœ¨å‰µå»ºæ‰¹æ¬¡è™•ç†ä»»å‹™...")
    
    batch = client.batches.create(
        input_file_id=input_file_id,
        endpoint="/v1/chat/completions",
        completion_window="24h",
        metadata={
            "description": "GSAT vocabulary polysemy analysis",
            "project": "vocab-website"
        }
    )
    
    print(f"âœ… æ‰¹æ¬¡ä»»å‹™å·²å‰µå»º")
    print(f"   Batch ID: {batch.id}")
    print(f"   ç‹€æ…‹: {batch.status}")
    
    return batch.id


def check_batch_status(batch_id: str) -> Dict[str, Any]:
    """æª¢æŸ¥æ‰¹æ¬¡è™•ç†ç‹€æ…‹"""
    batch = client.batches.retrieve(batch_id)
    
    status_info = {
        "id": batch.id,
        "status": batch.status,
        "created_at": batch.created_at,
        "completed_at": batch.completed_at,
        "failed_at": batch.failed_at,
        "expired_at": batch.expired_at,
        "output_file_id": batch.output_file_id,
        "error_file_id": batch.error_file_id,
        "request_counts": batch.request_counts
    }
    
    return status_info


def download_batch_results(output_file_id: str, output_path: Path) -> bool:
    """ä¸‹è¼‰æ‰¹æ¬¡è™•ç†çµæœ"""
    print(f"\næ­£åœ¨ä¸‹è¼‰æ‰¹æ¬¡çµæœ...")
    
    try:
        file_response = client.files.content(output_file_id)
        
        with open(output_path, 'wb') as f:
            f.write(file_response.content)
        
        print(f"âœ… çµæœå·²ä¸‹è¼‰åˆ°: {output_path}")
        return True
    except Exception as e:
        print(f"âŒ ä¸‹è¼‰å¤±æ•—: {e}")
        return False


def merge_polysemy_results(vocab_data: List[Dict[str, Any]], batch_output_path: Path) -> List[Dict[str, Any]]:
    """
    å°‡æ‰¹æ¬¡è™•ç†çµæœåˆä½µåˆ°è©å½™æ•¸æ“šä¸­
    """
    print(f"\næ­£åœ¨åˆä½µå¤šç¾©åˆ†æçµæœ...")
    
    # è®€å–æ‰¹æ¬¡è¼¸å‡º
    polysemy_results = {}
    
    with open(batch_output_path, 'r', encoding='utf-8') as f:
        for line in f:
            result = json.loads(line)
            custom_id = result.get("custom_id", "")
            
            if not custom_id.startswith("polysemy_"):
                continue
            
            lemma = custom_id.replace("polysemy_", "")
            
            # æª¢æŸ¥æ˜¯å¦æœ‰éŒ¯èª¤
            if result.get("error"):
                print(f"âš ï¸  å–®è© '{lemma}' è™•ç†å¤±æ•—: {result['error']}")
                continue
            
            # æå–éŸ¿æ‡‰å…§å®¹
            response = result.get("response", {})
            body = response.get("body", {})
            choices = body.get("choices", [])
            
            if not choices:
                print(f"âš ï¸  å–®è© '{lemma}' ç„¡éŸ¿æ‡‰å…§å®¹")
                continue
            
            content = choices[0].get("message", {}).get("content", "")
            
            try:
                meanings_data = json.loads(content)
                polysemy_results[lemma] = meanings_data.get("meanings", [])
            except json.JSONDecodeError as e:
                print(f"âš ï¸  å–®è© '{lemma}' çš„éŸ¿æ‡‰ç„¡æ³•è§£æç‚º JSON: {e}")
                continue
    
    print(f"âœ… æˆåŠŸè§£æ {len(polysemy_results)} å€‹å–®è©çš„å¤šç¾©åˆ†æçµæœ")
    
    # åˆä½µåˆ°åŸå§‹æ•¸æ“š
    enriched_count = 0
    for word_data in vocab_data:
        lemma = word_data["lemma"]
        
        if lemma in polysemy_results:
            word_data["meanings"] = polysemy_results[lemma]
            enriched_count += 1
        else:
            # å¦‚æœæ²’æœ‰æ‰¹æ¬¡çµæœï¼Œä½¿ç”¨åŸæœ‰çš„ç°¡å–®å®šç¾©
            word_data["meanings"] = []
            if "definition" in word_data and word_data["definition"]:
                # å°‡èˆŠæ ¼å¼è½‰æ›ç‚ºæ–°æ ¼å¼
                old_def = word_data["definition"]
                primary_pos = list(word_data.get("pos_dist", {}).keys())[0] if word_data.get("pos_dist") else "UNKNOWN"
                word_data["meanings"] = [{
                    "pos": primary_pos,
                    "zh_def": old_def.get("zh_def", ""),
                    "en_def": old_def.get("en_def", ""),
                    "example_indices": list(range(min(3, len(word_data.get("sentences", []))))),
                    "usage_note": old_def.get("example", "")
                }]
    
    print(f"âœ… å·²ç‚º {enriched_count} å€‹å–®è©æ·»åŠ å¤šç¾©åˆ†æ")
    
    return vocab_data


def main():
    """ä¸»åŸ·è¡Œå‡½å¼"""
    print("=" * 60)
    print("ä¸€å­—å¤šç¾©åˆ†æå·¥å…· (OpenAI Batch API)")
    print("=" * 60)
    
    # æª¢æŸ¥è¼¸å…¥æ–‡ä»¶
    if not VOCAB_JSON_PATH.exists():
        sys.exit(f"âŒ æ‰¾ä¸åˆ°è©å½™æ•¸æ“šæ–‡ä»¶: {VOCAB_JSON_PATH}")
    
    print(f"\nğŸ“– æ­£åœ¨è®€å–è©å½™æ•¸æ“š: {VOCAB_JSON_PATH}")
    with open(VOCAB_JSON_PATH, 'r', encoding='utf-8') as f:
        vocab_data = json.load(f)
    
    print(f"âœ… å·²è¼‰å…¥ {len(vocab_data)} å€‹å–®è©")
    
    # é¸æ“‡æ“ä½œæ¨¡å¼
    print("\nè«‹é¸æ“‡æ“ä½œæ¨¡å¼ï¼š")
    print("1. å‰µå»ºæ–°çš„æ‰¹æ¬¡ä»»å‹™")
    print("2. æª¢æŸ¥ç¾æœ‰æ‰¹æ¬¡ç‹€æ…‹")
    print("3. ä¸‹è¼‰ä¸¦åˆä½µæ‰¹æ¬¡çµæœ")
    
    choice = input("\nè«‹è¼¸å…¥é¸é … (1-3): ").strip()
    
    if choice == "1":
        # æ¨¡å¼ 1: å‰µå»ºæ–°æ‰¹æ¬¡
        input_file_path = create_batch_input_file(vocab_data)
        file_id = upload_batch_file(input_file_path)
        batch_id = create_batch_job(file_id)
        
        # ä¿å­˜æ‰¹æ¬¡ ID
        batch_info_path = BATCH_DIR / "batch_info.json"
        with open(batch_info_path, 'w', encoding='utf-8') as f:
            json.dump({
                "batch_id": batch_id,
                "input_file_id": file_id,
                "created_at": time.time(),
                "status": "submitted"
            }, f, indent=2)
        
        print(f"\nğŸ’¾ æ‰¹æ¬¡è³‡è¨Šå·²ä¿å­˜åˆ°: {batch_info_path}")
        print(f"\nâ° è«‹ç¨å¾Œä½¿ç”¨é¸é … 2 æª¢æŸ¥ç‹€æ…‹ï¼Œæˆ–é¸é … 3 ä¸‹è¼‰çµæœ")
        
    elif choice == "2":
        # æ¨¡å¼ 2: æª¢æŸ¥ç‹€æ…‹
        batch_info_path = BATCH_DIR / "batch_info.json"
        
        if not batch_info_path.exists():
            print("âŒ æ‰¾ä¸åˆ°æ‰¹æ¬¡è³‡è¨Šæ–‡ä»¶ï¼Œè«‹å…ˆå‰µå»ºæ‰¹æ¬¡ä»»å‹™")
            return
        
        with open(batch_info_path, 'r', encoding='utf-8') as f:
            batch_info = json.load(f)
        
        batch_id = batch_info.get("batch_id")
        if not batch_id:
            print("âŒ æ‰¹æ¬¡ ID ç„¡æ•ˆ")
            return
        
        print(f"\nğŸ” æ­£åœ¨æª¢æŸ¥æ‰¹æ¬¡ç‹€æ…‹: {batch_id}")
        status_info = check_batch_status(batch_id)
        
        print(f"\næ‰¹æ¬¡ç‹€æ…‹è³‡è¨Šï¼š")
        print(f"  ç‹€æ…‹: {status_info['status']}")
        print(f"  å‰µå»ºæ™‚é–“: {status_info['created_at']}")
        print(f"  å®Œæˆæ™‚é–“: {status_info['completed_at']}")
        print(f"  è«‹æ±‚çµ±è¨ˆ: {status_info['request_counts']}")
        
        if status_info['status'] == 'completed':
            print(f"\nâœ… æ‰¹æ¬¡è™•ç†å·²å®Œæˆï¼")
            print(f"  è¼¸å‡ºæ–‡ä»¶ ID: {status_info['output_file_id']}")
            print(f"\nè«‹ä½¿ç”¨é¸é … 3 ä¸‹è¼‰çµæœ")
            
            # æ›´æ–°æ‰¹æ¬¡è³‡è¨Š
            batch_info["status"] = "completed"
            batch_info["output_file_id"] = status_info['output_file_id']
            batch_info["completed_at"] = status_info['completed_at']
            with open(batch_info_path, 'w', encoding='utf-8') as f:
                json.dump(batch_info, f, indent=2)
        elif status_info['status'] in ['failed', 'expired', 'cancelled']:
            print(f"\nâŒ æ‰¹æ¬¡è™•ç†å¤±æ•—æˆ–è¢«å–æ¶ˆ")
            if status_info['error_file_id']:
                print(f"  éŒ¯èª¤æ–‡ä»¶ ID: {status_info['error_file_id']}")
        else:
            print(f"\nâ³ æ‰¹æ¬¡ä»åœ¨è™•ç†ä¸­ï¼Œè«‹ç¨å¾Œå†æª¢æŸ¥")
    
    elif choice == "3":
        # æ¨¡å¼ 3: ä¸‹è¼‰ä¸¦åˆä½µçµæœ
        batch_info_path = BATCH_DIR / "batch_info.json"
        
        if not batch_info_path.exists():
            print("âŒ æ‰¾ä¸åˆ°æ‰¹æ¬¡è³‡è¨Šæ–‡ä»¶")
            return
        
        with open(batch_info_path, 'r', encoding='utf-8') as f:
            batch_info = json.load(f)
        
        output_file_id = batch_info.get("output_file_id")
        
        if not output_file_id:
            # å˜—è©¦ç²å–æœ€æ–°ç‹€æ…‹
            batch_id = batch_info.get("batch_id")
            if batch_id:
                status_info = check_batch_status(batch_id)
                output_file_id = status_info.get('output_file_id')
        
        if not output_file_id:
            print("âŒ æ‰¹æ¬¡å°šæœªå®Œæˆæˆ–æ‰¾ä¸åˆ°è¼¸å‡ºæ–‡ä»¶ ID")
            print("   è«‹å…ˆä½¿ç”¨é¸é … 2 æª¢æŸ¥ç‹€æ…‹")
            return
        
        # ä¸‹è¼‰çµæœ
        if download_batch_results(output_file_id, BATCH_OUTPUT_PATH):
            # åˆä½µçµæœ
            enriched_vocab = merge_polysemy_results(vocab_data, BATCH_OUTPUT_PATH)
            
            # ä¿å­˜å¢å¼·å¾Œçš„è©å½™æ•¸æ“š
            with open(ENRICHED_VOCAB_PATH, 'w', encoding='utf-8') as f:
                json.dump(enriched_vocab, f, ensure_ascii=False, indent=2)
            
            print(f"\nè©å½™æ•¸æ“šå·²ä¿å­˜åˆ°: {ENRICHED_VOCAB_PATH}")
    
    else:
        print("âŒ ç„¡æ•ˆçš„é¸é …")


if __name__ == "__main__":
    main()

