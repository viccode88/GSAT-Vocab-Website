import json
import os
from pathlib import Path
import shutil

# --- Helper Functions from optimize_data_structure.py ---

def create_index_entry(word_data: dict, rank: int) -> dict:
    """Creates a lightweight index entry for the vocabulary list."""
    lemma = word_data["lemma"]
    count = word_data.get("count", 0)
    pos_dist = word_data.get("pos_dist", {})
    
    primary_pos = max(pos_dist, key=pos_dist.get) if pos_dist else ""
    
    meanings = word_data.get("meanings", [])
    meaning_count = len(meanings)
    
    zh_preview = ""
    en_preview = ""
    if meanings:
        zh_preview = meanings[0].get("zh_def", "")[:50]
        en_preview = meanings[0].get("en_def", "")[:50]
    
    return {
        "lemma": lemma,
        "count": count,
        "rank": rank,
        "primary_pos": primary_pos,
        "meaning_count": meaning_count,
        "zh_preview": zh_preview,
        "en_preview": en_preview
    }

def generate_search_index(vocab_data: list[dict]) -> dict:
    """Generates a search index for quick filtering on the frontend."""
    search_index = {
        "by_pos": {},
    }
    
    for idx, word_data in enumerate(vocab_data):
        lemma = word_data["lemma"]
        pos_dist = word_data.get("pos_dist", {})
        for pos in pos_dist.keys():
            if pos not in search_index["by_pos"]:
                search_index["by_pos"][pos] = []
            search_index["by_pos"][pos].append(lemma)
            
    return search_index

# --- Main Script ---

def main():
    """
    Splits the combined vocabulary data file into individual JSON files for each lemma,
    and also generates the vocabulary index and search index files.
    """
    print("=" * 60)
    print("è…³æœ¬ï¼šæº–å‚™ API è³‡æ–™ (æ‹†åˆ†è©³æƒ… + å»ºç«‹ç´¢å¼•)")
    print("=" * 60)

    # --- Path settings ---
    PROJECT_ROOT = Path(__file__).parent
    INPUT_FILE = PROJECT_ROOT / "data" / "output" / "vocab_data_filtered_cleaned_with_audio.json"
    OUTPUT_DIR_DETAILS = PROJECT_ROOT / "data" / "output" / "vocab_details"
    OUTPUT_FILE_INDEX = PROJECT_ROOT / "data" / "output" / "vocab_index.json"
    OUTPUT_FILE_SEARCH_INDEX = PROJECT_ROOT / "data" / "output" / "search_index.json"

    print(f"ğŸ“– è¼¸å…¥æª”æ¡ˆ: {INPUT_FILE}")
    print(f"ğŸ“‚ è¼¸å‡ºè©³æƒ…è³‡æ–™å¤¾: {OUTPUT_DIR_DETAILS}")
    print(f"ğŸ“‘ è¼¸å‡ºè©å½™ç´¢å¼•: {OUTPUT_FILE_INDEX}")
    print(f"ğŸ” è¼¸å‡ºæœå°‹ç´¢å¼•: {OUTPUT_FILE_SEARCH_INDEX}")

    # --- Check for input file ---
    if not INPUT_FILE.exists():
        print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°è¼¸å…¥æª”æ¡ˆã€‚è«‹å…ˆåŸ·è¡Œä¹‹å‰çš„è™•ç†æ­¥é©Ÿã€‚")
        return

    # --- Prepare output directory ---
    print(f"\nğŸ”„ æ­£åœ¨æº–å‚™è¼¸å‡ºè³‡æ–™å¤¾...")
    if OUTPUT_DIR_DETAILS.exists():
        print(f"   - æ­£åœ¨æ¸…ç©ºå·²å­˜åœ¨çš„è³‡æ–™å¤¾: {OUTPUT_DIR_DETAILS}")
        shutil.rmtree(OUTPUT_DIR_DETAILS)
    
    OUTPUT_DIR_DETAILS.mkdir(parents=True)
    print(f"   - âœ… å·²å»ºç«‹æ–°çš„è¼¸å‡ºè³‡æ–™å¤¾ã€‚")

    # --- Read data and process ---
    try:
        print("\nğŸ“š æ­£åœ¨è®€å–åˆä½µçš„è©å½™è³‡æ–™...")
        with open(INPUT_FILE, 'r', encoding='utf-8') as f:
            all_vocab_data = json.load(f)
        print(f"   - âœ… æˆåŠŸè®€å– {len(all_vocab_data)} å€‹è©å½™ã€‚")

        # --- Generate and save individual detail files ---
        print("\nâœï¸  æ­£åœ¨ç”Ÿæˆç¨ç«‹çš„è©³æƒ… JSON æª”æ¡ˆ...")
        details_count = 0
        for vocab_item in all_vocab_data:
            lemma = vocab_item.get("lemma")
            if not lemma:
                print(f"   - âš ï¸  è­¦å‘Šï¼šæ‰¾åˆ°ä¸€å€‹æ²’æœ‰ 'lemma' çš„é …ç›®ï¼Œå·²è·³éã€‚")
                continue

            safe_lemma = lemma.replace("/", "_")
            output_path = OUTPUT_DIR_DETAILS / f"{safe_lemma}.json"
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(vocab_item, f, indent=2, ensure_ascii=False)
            details_count += 1
        print(f"   - âœ… æˆåŠŸç”Ÿæˆ {details_count} å€‹è©³æƒ…æª”æ¡ˆï¼")

        # --- Generate and save vocabulary index ---
        print("\nğŸ“‘ æ­£åœ¨ç”Ÿæˆè©å½™ç´¢å¼•...")
        vocab_index = [create_index_entry(word, i + 1) for i, word in enumerate(all_vocab_data)]
        with open(OUTPUT_FILE_INDEX, 'w', encoding='utf-8') as f:
            json.dump(vocab_index, f, indent=2, ensure_ascii=False)
        print(f"   - âœ… è©å½™ç´¢å¼•å·²å„²å­˜è‡³: {OUTPUT_FILE_INDEX}")

        # --- Generate and save search index ---
        print("\nğŸ” æ­£åœ¨ç”Ÿæˆæœå°‹ç´¢å¼•...")
        search_idx = generate_search_index(all_vocab_data)
        with open(OUTPUT_FILE_SEARCH_INDEX, 'w', encoding='utf-8') as f:
            json.dump(search_idx, f, indent=2, ensure_ascii=False)
        print(f"   - âœ… æœå°‹ç´¢å¼•å·²å„²å­˜è‡³: {OUTPUT_FILE_SEARCH_INDEX}")

        print(f"\nğŸ‰ è™•ç†å®Œæˆï¼æ‰€æœ‰ API è³‡æ–™å·²æº–å‚™å°±ç·’ã€‚")

    except json.JSONDecodeError:
        print(f"âŒ éŒ¯èª¤ï¼šè¼¸å…¥æª”æ¡ˆ '{INPUT_FILE}' ä¸æ˜¯ä¸€å€‹æœ‰æ•ˆçš„ JSON æª”æ¡ˆã€‚")
    except Exception as e:
        print(f"âŒ ç™¼ç”Ÿæœªé æœŸçš„éŒ¯èª¤: {e}")

if __name__ == "__main__":
    main()
